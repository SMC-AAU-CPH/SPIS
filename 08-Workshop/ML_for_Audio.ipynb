{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q82jF4Im84hk"
      },
      "source": [
        "# SPIS Workshop\n",
        "## Audio Processing from a Machine Learning Perspective\n",
        "### **Differentiable Digital Signal Processing (DDSP)**\n",
        "#### *Anders R. Bargum (PhD Student)*, Cumhur Erkut, Monday 7th of April, 2025\n",
        "\n",
        "Welcome to the main notebook of the workshop \"*Audio Processing from a Machine Learning Perspective*\", created for the \"*Signal Processing for Interactive Systems*\" course at Aalborg University Copenhagen. During this workshop we will cover how machine learning principles can be applied to audio, specifically for audio-effects and audio-processing. Some of you may be familiar with \"neural networks\", which are models that operate on an input using different layers of functions, additions and multiplications in order to predict a specific target. This workshop is <ins>NOT</ins> a walkthrough of neural networks. Rather, we will look at machine-learning principles from a signal-based approach, more specifically called \"*Differential Digital Signal Processing*\" (DDSP).\n",
        "\n",
        "<div>\n",
        "<img src=\"./img/abstract.png\" width=\"750\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "This notebook will cover the following:\n",
        "\n",
        "- **Introduction:** What is DDSP and how can we use it?\n",
        "- **PyTorch and differentiability:** A quick recap\n",
        "- **Toy problem:** A differentiable gain control\n",
        "- **Loss functions**: L1, MSE, Spectral Loss\n",
        "- **Other use-cases:** Filter design, waveshaping etc.\n",
        "- **Optimizing physical models:** Applicable to SMC physical modelling class\n",
        "\n",
        "Much of this notebook is based on the workshop \"*Introduction to DDSP for Audio Synthesis*\" by Ben Hayes, Jordie Shier, Chin-Yun Yu, David Südholt, Rodrigo Diaz (https://intro2ddsp.github.io/intro.html#).\n",
        "\n",
        "I additionally refer you to the following work for more information:\n",
        "\n",
        "- DDSP, Differentiable Digital Signal Processing: https://magenta.tensorflow.org/ddsp (2019)\n",
        "- Kuznetsov et. al: Differentiable IIR Filters for Machine Learning Applications (2020)\n",
        "- Hayes et. al: A Review of Differentiable Digital Signal Processing for Music & Speech Synthesis (2023)\n",
        "- Steinmetz et. al: Style Transfer of Audio Effects with Differentiable Signal Processing https://csteinmetz1.github.io/DeepAFx-ST/ (2022)\n",
        "- Bargum et. al: Differentiable Allpass Filters for Phase Response Estimation and Automatic Signal Alignment (2023)\n",
        "\n",
        "Let's start by installing any needed packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IV-qJF-9WM3L",
        "outputId": "d9a9112d-a4fd-403b-9864-ebad63780f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-07 13:09:58--  https://raw.githubusercontent.com/SMC-AAU-CPH/SPIS/refs/heads/main/08-Workshop/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3628 (3.5K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   3.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-07 13:09:58 (32.7 MB/s) - ‘utils.py’ saved [3628/3628]\n",
            "\n",
            "--2025-04-07 13:09:58--  https://raw.githubusercontent.com/SMC-AAU-CPH/SPIS/refs/heads/main/08-Workshop/sound-files/guitar.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1583068 (1.5M) [audio/wav]\n",
            "Saving to: ‘sound-files/guitar.wav’\n",
            "\n",
            "guitar.wav          100%[===================>]   1.51M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-07 13:09:59 (35.2 MB/s) - ‘sound-files/guitar.wav’ saved [1583068/1583068]\n",
            "\n",
            "--2025-04-07 13:09:59--  https://raw.githubusercontent.com/SMC-AAU-CPH/SPIS/refs/heads/main/08-Workshop/sound-files/guitar-nsynth.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128044 (125K) [audio/wav]\n",
            "Saving to: ‘sound-files/guitar-nsynth.wav’\n",
            "\n",
            "guitar-nsynth.wav   100%[===================>] 125.04K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-04-07 13:09:59 (7.34 MB/s) - ‘sound-files/guitar-nsynth.wav’ saved [128044/128044]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  !wget https://raw.githubusercontent.com/SMC-AAU-CPH/SPIS/refs/heads/main/08-Workshop/utils.py\n",
        "  !mkdir -p sound-files\n",
        "  !wget https://raw.githubusercontent.com/SMC-AAU-CPH/SPIS/refs/heads/main/08-Workshop/sound-files/guitar.wav -P sound-files\n",
        "  !wget https://raw.githubusercontent.com/SMC-AAU-CPH/SPIS/refs/heads/main/08-Workshop/sound-files/guitar-nsynth.wav -P sound-files\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OnWvOiTc84hm",
        "outputId": "2a6765ae-146a-4bc5-8019-c3c597b3d8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install librosa\n",
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPDfW8ou84hn"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sykVnX8784hn"
      },
      "outputs": [],
      "source": [
        "from utils import plot_graph, get_sine, plot_tf, DIIRDataSet, DIIR_WRAPPER\n",
        "import IPython.display as ipd\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn import Module, Parameter\n",
        "from torch import FloatTensor\n",
        "from numpy.random import uniform\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "import librosa\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VinofphF84hn",
        "outputId": "86e5590c-4a01-4c08-a309-f7ec6a148524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = False\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "  else:\n",
        "    device = \"cpu\"\n",
        "  print(\"Using device:\", device)\n",
        "except:\n",
        "  IN_COLAB = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O5Gnczg84ho"
      },
      "source": [
        "## Introduction: Differential Digital Signal Processing (DDSP)\n",
        "\n",
        "From a mathematical perspective a *differentiable* function is a function whose derivative exists at all points in its domain i.e. that we can take the derivate of function $f(x)$ no matter what value $x$ takes. In short, when we differentiate we find  the rate of change of a function $f(x)$ with respect to its input $x$ (the rise in $y$ with respect to the rise in $x$). Basically, we are finding the slope of the tangent line at the specific point of $x$. When the slope of the tangent is 0, it indicates a critical point, which could be a minimum, maximum, or a saddle point. In many ML cases we want to find the minimum of a function.\n",
        "\n",
        "<div>\n",
        "<img src=\"./img/derivative.png\" width=\"350\"/>\n",
        "</div>\n",
        "\n",
        "#### **In a machine learning context I like to think of <ins>differentiable</ins> as something that is \"updatable\"**.\n",
        "\n",
        "In a signal processing manner, this means that we can take a signal processor with different parameters (being a function), and approximate the specific parameters of a given behaviour. We do this by implementing the function and automatically updating the parameters such that we reach a specific minimum (can only be done if the signal processing function itself is differentiable). With this in mind, we can implement a signal processor, or a chain of different signal processors, and automatically update its internal parameters using an automatic differentiation framework such as TensorFlow, PyTorch, or Jax (which does all the troublesome differentiation-work for you).\n",
        "\n",
        "Lets look at an example:\n",
        "\n",
        "### A simple **Linear Gain** Effect\n",
        "\n",
        "A linear gain effect is a really simple and good example of how differentiable signal processing works, as it presents a differentiable system (we can take the derivative of $f(x)$) with an obvious parameter (the gain factor $g$). Normally, it would be very easy for us as developers just to change the value $g$ to find the desired gain.\n",
        "\n",
        "But what if the system was part of a larger sub-system? In that case it would not be as easy.\n",
        "Or what if we were to create a specific frequency response using a filter with coefficients a1, a2, a3, b1, b2, b3? Then it would also be difficult to tune the coefficients by hand.\n",
        "\n",
        "In the case of the linear gain effect, we can finde the gain value $g$ using differentiability.\n",
        "\n",
        "We need:\n",
        "\n",
        "1. An input, could be anything from a sine wave to a complex instrument signal\n",
        "2. An output, the same as the input signal but affected by the system we want to approximate (lets say at half the amplitude value of the input)\n",
        "3. The system we want to approximate, implemented for differentiation (using <ins>nn.Module</ins> in PyTorch)\n",
        "4. A loss function that can compare how far our target is from our prediction (this is the function we want to find the minimum of)\n",
        "5. Gradients telling us how far the given parameters are from minimizing the loss function\n",
        "\n",
        "Following the pipeline below, we can then recursively keep updating gain parameter $g$ until the output of the gain-system matches that of the target.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0TzK4Tz84ho"
      },
      "source": [
        "<div>\n",
        "<img src=\"./img/ddsp.png\" width=\"1000\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxWN-2mH84ho"
      },
      "source": [
        "# PyTorch and Differentiability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m21hB0DQ84ho"
      },
      "source": [
        "To implement above system we can use PyTorch. PyTorch provides many utilities around neural networks and deep learning, but at its very core it consists of two main features: GPU-accelerated linear algebra operations, and automatic differentiation.\n",
        "\n",
        "Let’s quickly recap the basics of PyTorch.\n",
        "\n",
        "## Tensors\n",
        "\n",
        "Tensors are the basic data structure in PyTorch. They are similar to numpy arrays, but offer support for the two main features mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SMrcGKhf84ho",
        "outputId": "92586471-4b3a-46a2-adb6-ab4368abefcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = tensor(42.)\n",
            "v = tensor([1., 2., 3.])\n",
            "M = tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n"
          ]
        }
      ],
      "source": [
        "# Create a scalar (0D tensor)\n",
        "scalar = torch.tensor(42, dtype=torch.float32) # explicitly enforce float type\n",
        "print(\"a =\", scalar)\n",
        "\n",
        "# Create a vector (1D tensor)\n",
        "vector = torch.tensor([1., 2., 3.])\n",
        "print(\"v =\", vector)\n",
        "\n",
        "# Create a matrix (2D tensor)\n",
        "matrix = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(\"M =\", matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVzuuUMN84hp"
      },
      "source": [
        "Basic arithmetic operations are applied element-wise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQS2GOW484hp"
      },
      "outputs": [],
      "source": [
        "print(\"v + v =\", vector + vector) #addition of two vectors\n",
        "print(\"v * v =\", vector * vector) #multiplication of two vectors (element-wise)\n",
        "print(\"M * v =\", matrix * vector) #broadcasting (expanding dimensions automatically)\n",
        "\n",
        "print(\"M x v\", torch.matmul(matrix, vector)) #matrix multiplication/dot product ((2x3) * (3x1) = (2x1))\n",
        "print(\"M @ v =\", matrix @ vector) #matrix multiplication/dot product ((2x3) * (3x1) = (2x1))\n",
        "\n",
        "print(\"v^2 =\", vector ** 2)  #take the power of 2 element-wise\n",
        "print(\"exp(v) =\", torch.exp(vector)) #take the exponent element-wise\n",
        "print(\"sin(v) =\", torch.sin(vector)) #take the sine element-wise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSuMW6dU84hp"
      },
      "source": [
        "## Gradients and Auto-Differentiation\n",
        "\n",
        "As in the linear gain example, we are interested in a value that minimize the difference between the system-output and the target (also called the loss or the error). In essence, the loss is an objective function that we try to minimize. In order to do this we can use automatic differentiation: if we know the gradient of a function with respect to its inputs (the slope of all tangent lines), we know that adjusting the inputs in the opposite direction of the gradient will decrease the value of the function and go towards a minimum. This is called gradient descent optimization.\n",
        "\n",
        "To let PyTorch know that we want to compute the gradient with respect to a certain tensor, we need to set the requires_grad flag. Let’s take a look at what happens when we do this and perform operations on the tensor to calculate: y = g * x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y776wDTB84hp"
      },
      "outputs": [],
      "source": [
        "#We initalise the input at a random value\n",
        "x = torch.tensor(0.1, requires_grad=True)\n",
        "\n",
        "#We intialise g at a random value\n",
        "g = torch.tensor(0.8, requires_grad=True)\n",
        "\n",
        "#We define the function we want to optimise\n",
        "y = g * x\n",
        "\n",
        "#We see that every tensor now carries an attribute grad_fn that describes how to compute the gradient of the operation that it resulted from.\n",
        "print(\"g:\", g)\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "\n",
        "# In the backward pass, this graph is used to compute the gradient of the final output with respect to the initial inputs.\n",
        "# The backward pass can be triggered by calling the backward() method on the final output.\n",
        "y.backward()\n",
        "print(\"dy/dx evaluated at g=0.8:\", x.grad.numpy())\n",
        "\n",
        "print(\"\\nThis makes sense as the derivative of our function g * x with respect to x equals to g\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTEFV7la84hp"
      },
      "source": [
        "Lets look at a more \"complicated\" example, with function:\n",
        "\n",
        "$\\begin{aligned} z = sin((w + 1)^3) \\end{aligned}$.\n",
        "\n",
        "We can either split the function into sub-computations - using the chain rule from calculus we can calculate the gradient of the final output with respect to the input by decomposing it into a product of gradients from the sub-computations:\n",
        "\n",
        "$\\dfrac{dz}{dw} = \\dfrac{dz}{dy} \\cdot \\dfrac{dy}{dx} \\cdot \\dfrac{dx}{dw}$\n",
        "\n",
        "Or, we can do it directly on the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfvki-hI84hp"
      },
      "outputs": [],
      "source": [
        "#sub computations\n",
        "w = torch.tensor(1., requires_grad=True)\n",
        "x = w + 1\n",
        "y = x ** 3\n",
        "z = torch.sin(y)\n",
        "z.backward()\n",
        "print(\"dz/dw evaluated at w=1:\", w.grad)\n",
        "\n",
        "#directly\n",
        "w = torch.tensor(1., requires_grad=True)\n",
        "z = torch.sin(torch.pow((w+1),3))\n",
        "z.backward()\n",
        "print(\"dz/dw evaluated at w=1:\", w.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B3JPHv184hp"
      },
      "source": [
        "This is the essence of auto-differentiation. In the forward pass, PyTorch builds a computational graph of operations that know how to compute their gradients locally (as done in the sub-computations). In the backward pass, this graph is used to compute the gradient of the final output with respect to the initial inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVARaKjP84hp"
      },
      "source": [
        "# Optimizers\n",
        "\n",
        "Now that we know how to compute gradients, we can use them to find the parameters that minimize some objective function. In the most basic version of gradient descent, we update our estimate of parameters of a function according to the following rule:\n",
        "\n",
        "$x \\leftarrow x - \\lambda \\nabla_x f(x)$\n",
        "\n",
        "Where x is the parameter we want to update, $\\nabla_x f(x)$ is the gradient of the function $f$ with respect to x, and $\\lambda$ is a small scalar defining how much we want to update our parameter based on the gradient (often very small, as we do want to overshoot). $\\lambda$ is also called the \"learning rate\".\n",
        "\n",
        "The function above is called Stochastic Gradient Descent (SGD), whereas the update itself is called a \"step\". **SGD** and **step** are automatically implemented in PyTorch.\n",
        "\n",
        "*As seen earlier, the gradient points into the direction of steepest ascent (increase), so we need to subtract it from $x$ to move in the direction of steepest descent (decrease), i.e. towards the minimum of $f$ (remember that the minimum is where the loss is 0 i.e. where the processed input is similar to the target).*\n",
        "\n",
        "#### **Let's look at a simple example of finding the minimum of a function.**\n",
        "We use the function $f(x) = x^2 - 4x + 2x - 1$\n",
        "\n",
        "Analytically we can find the minimum of $f(x)$ by taking its derivative, setting it to zero and solving for x:\n",
        "- $\\frac{dy}{dx} = 2x - 4 + 2$\n",
        "- $0 = 2x - 4 + 2 \\rightarrow x = 1$\n",
        "\n",
        "Let’s try to find the same minimum using gradient descent and automatic differentiation. More specifically we update the parameters based on the gradient of x with respect to y iteratively. As a default value we choose 500 iterations (we can also experiment with different learning rates and see how it affects the updated parameter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrbEahdK84hp"
      },
      "outputs": [],
      "source": [
        "# Initial estimate\n",
        "x = torch.tensor(0., requires_grad=True)\n",
        "# Initialize optimizer with parameters to be optimized and learning rate \"lambda\"\n",
        "optim = torch.optim.SGD([x], lr=0.01)\n",
        "# Number of iterations\n",
        "n_iter = 500\n",
        "\n",
        "#create list to track value of x\n",
        "xs = []\n",
        "#ys = []\n",
        "\n",
        "# Gradient descent loop\n",
        "for i in range(n_iter):\n",
        "\n",
        "    # Logging\n",
        "    xs.append(x.item())\n",
        "\n",
        "    f = x**2 - 4*x + 2*x - 1 # forward pass\n",
        "    f.backward() # backward pass\n",
        "\n",
        "    #ys.append(f.item())\n",
        "\n",
        "    optim.step() # perform the gradient descent step\n",
        "    optim.zero_grad() # reset the gradients\n",
        "\n",
        "# Plot how the estimate for x converged\n",
        "plot_graph('Iteration', 'Estimate of x', [-0.5, 2.0], xs)\n",
        "#plot_graph('Iteration', 'Estimate of x', [-3.5, 2.0], ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7CX7Imr84hp"
      },
      "source": [
        "We clearly see that through differentation and and automatic update, we approximate the value of x that gives us the minimum of our function $f(x)$.\n",
        "\n",
        "Now we will start to implement this in an actual-example. Please note that we now will start to compare the processed output towards a target using a specific loss function. This means that we are trying to optimize the loss function and not the DSP function. However, the DSP function still needs to be differentiable simply because this allows us to retrieve and track gradients as well as update the parameters in the backward() call. </ins> ####\n",
        "\n",
        "All loss-functions provided in PyTorch are differentiable too.\n",
        "\n",
        "Lets take the learned components and combine them to create a differentiable linear gain model that can predict the gain value for a specific input output pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lotl1nX984hp"
      },
      "source": [
        "### A simple <ins>*Differentiable*</ins> **Linear Gain** Effect\n",
        "\n",
        "We will now take a pair of dry audio and wet audio processed by a gain factor. We will use gradient descent to estimate the parameters that were applied to the dry signal to obtain the processed one. We can inherit the nn.Module and Parameter classes from PyTorch to define the behaviour and parameters of our gain control.\n",
        "\n",
        "We start by creating our train and target signals.\n",
        "#### **The gain value we want to predict is 0.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSsH7SoL84hq"
      },
      "outputs": [],
      "source": [
        "#Create input and target\n",
        "sr = 16000\n",
        "freq = 200\n",
        "target_gain = 0.2\n",
        "\n",
        "# generate half a second of sine wave at 300 Hz\n",
        "input_audio = get_sine(1.0, freq, sr)\n",
        "target_audio = get_sine(target_gain, freq, sr)\n",
        "\n",
        "plot_graph('Sample', 'Amplitude', [-1.2, 1.2], input_audio[:200], target_audio[:200], [\"Target\", \"Original\"])\n",
        "\n",
        "ipd.display(ipd.Audio(input_audio, rate=sr, normalize=False))\n",
        "ipd.display(ipd.Audio(target_audio, rate=sr, normalize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uqP5_Rb84hq"
      },
      "source": [
        "We define our model using nn.Module (see that it inherits a forward function), and train. The forward() function of nn.Module stores all values and computations making sure that we can call backward() where we calculate the gradients used for gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWWGwRKQ84hq"
      },
      "outputs": [],
      "source": [
        "#create linear gain function\n",
        "class LinearGain(torch.nn.Module):\n",
        "    def __init__(self, gain=1.0):\n",
        "        super().__init__()\n",
        "        self.gain = torch.nn.Parameter(torch.tensor(gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gain * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZJiiiaF84hq"
      },
      "source": [
        "We initialise the LinearGain class, a loss function we want to use to compare the processed input and target, as well as the SGD optimiser. We train for 300 iterations. Note that we use optim.zero_grad() for each iteration. This ensures that all gradient values are swiped for every parameter update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfcmkMRc84hq"
      },
      "outputs": [],
      "source": [
        "diff_gain = LinearGain() # initialise module\n",
        "l1_loss = torch.nn.L1Loss() # measures the mean absolute error (MAE) between each element in the input x and target\n",
        "optim = torch.optim.SGD(diff_gain.parameters(), lr=0.01) #initialise optimizer\n",
        "\n",
        "n_iter = 300\n",
        "\n",
        "gains = []\n",
        "losses = []\n",
        "\n",
        "for i in range(n_iter):\n",
        "    #logging\n",
        "    gains.append(diff_gain.gain.item())\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    estim_audio = diff_gain(input_audio) # forward pass\n",
        "\n",
        "    loss = l1_loss(estim_audio, target_audio)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward() #calculate gradients based on loss\n",
        "\n",
        "    optim.step() #update the parameter\n",
        "\n",
        "# Plot how the estimate for x converged\n",
        "plot_graph('Iteration', 'Loss', [-0.2, 0.8], losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKB4p7JP84hq"
      },
      "source": [
        "Lets look at the how the processed signal changes for each iteration compared to the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1jsYmXW84hq"
      },
      "outputs": [],
      "source": [
        "# Animate the fitting process\n",
        "def get_gain_loss_animation(org, tgt, gains, losses):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Plot target and estimate\n",
        "    ax[0].plot(tgt[:200])\n",
        "    line, = ax[0].plot([], [])\n",
        "    ax[0].set_xlabel(\"Time (samples)\")\n",
        "    ax[0].set_ylabel(\"Amplitude\")\n",
        "    ax[0].set_ylim(-1, 1)\n",
        "    ax[0].legend([\"Target\", \"Estimate\"], loc=1)\n",
        "\n",
        "    # Plot losses animation\n",
        "    ax[1].set_xlim(0, len(losses))\n",
        "    ax[1].set_ylim(min(losses), max(losses))\n",
        "    line_loss, = ax[1].plot([], [], lw=2)\n",
        "    ax[1].set_xlabel(\"Iteration\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "\n",
        "    def init():\n",
        "        line.set_data([], [])\n",
        "        line_loss.set_data([], [])\n",
        "        return line, line_loss,\n",
        "\n",
        "    def animate(i):\n",
        "        # Update estimate plot\n",
        "        line.set_data(np.arange(200), org[:200] * gains[i * 5])\n",
        "        ax[0].set_title(f\"Estimated signal after {i * 5} iterations\")\n",
        "        ax[1].set_title(f\"Loss after {i * 5} iterations\")\n",
        "\n",
        "        # Update losses plot\n",
        "        line_loss.set_data(np.arange((i*5)+1), losses[:(i*5)+1])\n",
        "        return line, line_loss,\n",
        "\n",
        "    # Create the animation\n",
        "    anim = FuncAnimation(fig, animate, init_func=init, frames=len(gains) // 5, interval=50, blit=True)\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "display(HTML(get_gain_loss_animation(input_audio, target_audio, gains, losses).to_html5_video()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrbbPZ8N84hq"
      },
      "source": [
        "We see that we over time learn the gain value that results in the target. We also see that the loss, being the mean squared amplitude difference between the processed input and the target, decreases and reaches 0 at the same iteration that the sines perfectly align in amplitude.\n",
        "\n",
        "However, the problem we are trying to solve is very easy. Simply because it is linear, contains input and target that is aligned time-wise and only includes one parameter.\n",
        "\n",
        "Rather, let's look at something a bit more complex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zleo00KZ84hq"
      },
      "source": [
        "### A simple <ins>*Differentiable*</ins> **Waveshaper**\n",
        "\n",
        "A waveshaper is used to shape a sound giving it more harmonics. This often results in a warm or harsh feeling also known from saturation or overdrive effects (espesically prominent in guitar pedals). If we want to model the characteristics of analog distortion, and especially tube distortion, we can use a modified tanh() function as this allows us to model the positive and negative slopes of the input differently. The modified tanh function is given by:\n",
        "\n",
        "$\\begin{aligned}tanh_{mod}(x) = \\frac{e^{x*(a+G)} - e^{x*(b+G)}}{e^{x*G} + e^{x*-G}}\\end{aligned}$\n",
        "\n",
        "Here the distortion amount $G$ defines the overall shape/drive, whereas $a$ and $b$ are small offsets added to the positive and negative parts of the input signal respectively.\n",
        "\n",
        "Lets create a training and random target signal using the modified tanh function. The target could here also be an analog tube distortion effect that you do not know the inner workings of. Using DDSP you could try to model it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd9tbykH84hq"
      },
      "outputs": [],
      "source": [
        "def mod_tanh(x, a, b, g):\n",
        "    numerator = np.exp(x*(a+g)) - np.exp(x*(b-g))\n",
        "    denominator = np.exp(x*g) + np.exp(x*(-g))\n",
        "    return numerator/denominator\n",
        "\n",
        "#Create input and target\n",
        "sr = 48000\n",
        "freq = 300\n",
        "\n",
        "target_a = 0.6\n",
        "target_b = 0.4\n",
        "target_g = 4.5\n",
        "\n",
        "# generate half a second of sine wave at 300 Hz\n",
        "input_audio = get_sine(1.0, freq, sr)\n",
        "target_audio = mod_tanh(input_audio, target_a, target_b, target_g)\n",
        "\n",
        "plot_graph('Sample', 'Amplitude', [-1.2, 3.2], input_audio[:200], target_audio[:200], [\"Target\", \"Original\"])\n",
        "\n",
        "ipd.display(ipd.Audio(input_audio, rate=sr, normalize=True))\n",
        "ipd.display(ipd.Audio(target_audio, rate=sr, normalize=True))\n",
        "\n",
        "guitar, _ = librosa.load('sound-files/guitar.wav', sr=sr, mono=True)\n",
        "guitar_dist = mod_tanh(guitar, target_a, target_b, target_g)\n",
        "ipd.display(ipd.Audio(guitar, rate=sr, normalize=True))\n",
        "ipd.display(ipd.Audio(guitar_dist, rate=sr, normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QoUG0SK84hq"
      },
      "source": [
        "We implement it in the PyTorch framework for automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfTpJmh084hq"
      },
      "outputs": [],
      "source": [
        "class Modified_Tanh(torch.nn.Module):\n",
        "    def __init__(self, a=0.0, b=0.0, g=0.0):\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.tensor(a))\n",
        "        self.b = torch.nn.Parameter(torch.tensor(b))\n",
        "        self.g = torch.nn.Parameter(torch.tensor(g))\n",
        "\n",
        "    def forward(self, x):\n",
        "        numerator = torch.exp(x*(self.a+self.g)) - torch.exp(x*(self.b-self.g))\n",
        "        denominator = torch.exp(x*self.g) + torch.exp(x*(-self.g))\n",
        "        return numerator/denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T32Jk56M84hq"
      },
      "source": [
        "Again, we train using the SGD optimizer to find the waveshaping values we applied to the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBnxdsk884hq"
      },
      "outputs": [],
      "source": [
        "diff_effect = Modified_Tanh() # initialise module\n",
        "l1_loss = torch.nn.L1Loss() # measures the mean absolute error (MAE) between each element in the input x and target\n",
        "optim = torch.optim.SGD(diff_effect.parameters(), lr=0.01) #initialise optimizer\n",
        "\n",
        "n_iter = 5000\n",
        "\n",
        "a = []\n",
        "b = []\n",
        "g = []\n",
        "losses = []\n",
        "\n",
        "for i in range(n_iter):\n",
        "    #logging\n",
        "    a.append(diff_effect.a.item())\n",
        "    b.append(diff_effect.b.item())\n",
        "    g.append(diff_effect.g.item())\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    estim_audio = diff_effect(input_audio) # forward pass\n",
        "\n",
        "    loss = l1_loss(estim_audio, target_audio)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward() #calculate gradients based on loss\n",
        "\n",
        "    optim.step() #update the parameter\n",
        "\n",
        "# Plot how the estimate for x converged\n",
        "plot_graph('Iteration', 'Loss', [-0.2, 0.8], losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ41wKGC84hr"
      },
      "source": [
        "We can now track the parameters/coefficients to see how they change over time when we update them through gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThwrKB-C84hr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Animate the fitting process\n",
        "def get_loss_animation(losses_list):\n",
        "    num_iterations = len(losses_list[0])\n",
        "    num_plots = len(losses_list)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "    lines_loss = []\n",
        "    annotations = []\n",
        "    for i in range(num_plots):\n",
        "        if i == 0:\n",
        "            label = \"a\"\n",
        "        elif i == 1:\n",
        "            label = \"b\"\n",
        "        else:\n",
        "            label = \"g\"\n",
        "\n",
        "        line_loss, = ax.plot([], [], lw=2, label=label)\n",
        "        lines_loss.append(line_loss)\n",
        "        annotations.append(ax.text(0.95, 0.9-i*0.1, '', transform=ax.transAxes, ha='right', va='center'))\n",
        "\n",
        "    ax.set_xlim(0, num_iterations)\n",
        "    ax.set_ylim(0, max([max(losses) for losses in losses_list]))\n",
        "    ax.set_xlabel(\"Iteration\")\n",
        "    ax.set_ylabel(\"Param Val\")\n",
        "    ax.legend(loc='upper left')\n",
        "\n",
        "    def init():\n",
        "        for line, annotation in zip(lines_loss, annotations):\n",
        "            line.set_data([], [])\n",
        "            annotation.set_text('')\n",
        "        return lines_loss + annotations\n",
        "\n",
        "    def animate(iter_idx):\n",
        "        for i, (line_loss, annotation) in enumerate(zip(lines_loss, annotations)):\n",
        "            if i == 0:\n",
        "                label = \"a\"\n",
        "                target = target_a\n",
        "            elif i == 1:\n",
        "                label = \"b\"\n",
        "                target = target_b\n",
        "            else:\n",
        "                label = \"g\"\n",
        "                target = target_g\n",
        "\n",
        "            line_loss.set_data(np.arange((iter_idx*30)+1), losses_list[i][:(iter_idx*30)+1])\n",
        "            annotation.set_text(f\"{label}: {losses_list[i][iter_idx*30]:.2f} - target: {target}\")\n",
        "            annotation.set_position((0.95, 0.9-i*0.1))\n",
        "            ax.set_title(f\"Parameters after {iter_idx * 30} iterations\")\n",
        "        return lines_loss + annotations\n",
        "\n",
        "    # Create the animation\n",
        "    anim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations // 30, interval=50, blit=True)\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "coeffs = [a, b, g]\n",
        "\n",
        "# Example usage:\n",
        "display(HTML(get_loss_animation(coeffs).to_html5_video()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB7RlP4M84hr"
      },
      "source": [
        "We are pretty close! Lets try to hear it on a guitar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN_hd_VX84hr"
      },
      "outputs": [],
      "source": [
        "diff_effect.eval()\n",
        "\n",
        "input_to_process = torch.tensor(guitar)\n",
        "\n",
        "with torch.no_grad():\n",
        "    processed = diff_effect(input_to_process)\n",
        "\n",
        "processed = processed.reshape(-1).cpu().numpy()\n",
        "print(\"Original\")\n",
        "ipd.display(ipd.Audio(guitar, rate=sr, normalize=True))\n",
        "print(\"Target\")\n",
        "ipd.display(ipd.Audio(guitar_dist, rate=sr, normalize=True))\n",
        "print(\"Predicted\")\n",
        "ipd.display(ipd.Audio(processed, rate=sr, normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBI24msn84hu"
      },
      "source": [
        "### The art of choosing the right Loss function\n",
        "\n",
        "There exists many different loss-functions, operating in different ways. Until now we have only used the L1 loss (also called MAE) that measures the average distance between the absolute values of our output and target:\n",
        "\n",
        "$\\begin{aligned}L1=\\sum_{i=1}^n\\left|y_{\\text {true }}-y_{\\text {predicted }}\\right|\\end{aligned}$\n",
        "\n",
        "Many other loss functions exist, with each their behaviour. Below we see the different loss functions in 2D. As we see the L1/MAE is not differentiable at the minima.\n",
        "\n",
        "<div>\n",
        "<img src=\"./img/losses.png\" width=\"450\"/>\n",
        "</div>\n",
        "\n",
        "Until now we’ve made the tasks a bit easy for ourselves. What if the signal we’re trying to match differs from the processed signal in more ways than just the shape or gain? What will happen if we phase-shift the target signal by 180 degree?\n",
        "\n",
        "Using the L1 loss, as done until now, will most likely have troubles. Although a phase shift changes nothing about the human perception of the sound, the loss function we use will not be sufficient in comparing the signals (simply because we are comparing the signals data-point by data-point). The loss will thus no longer deliver gradients that point us in the correct direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zJuGkus84hu",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "sr = 16000\n",
        "freq = 300\n",
        "true_gain = 0.15\n",
        "\n",
        "# generate half a second of sine wave at 300 Hz\n",
        "input_audio = get_sine(1.0, freq, sr)\n",
        "target_sine = torch.cos(torch.linspace(0, 2 * torch.pi * freq, sr // 2))\n",
        "target_audio = true_gain * target_sine\n",
        "\n",
        "plot_graph('Sample', 'Amplitude', [-1.2, 1.2], input_audio[:200], target_audio[:200], [\"Original\", \"Target\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bexYBT4h84hu"
      },
      "source": [
        "The correct choice of the loss function can play a crucial role in optimizing parameters for audio controls. In this case, we want the loss to be invariant to phase shifts. In the case of gain, we can as an example compute the spectrogram and compare the magnitudes of the frequency bins.\n",
        "\n",
        "We can write a custom loss module to do just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vx-ILgZ84hv"
      },
      "outputs": [],
      "source": [
        "class SpectralLoss(torch.nn.Module):\n",
        "    def __init__(self, power=1):\n",
        "        super().__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_mags = torch.fft.rfft(x).abs() ** self.power\n",
        "        y_mags = torch.fft.rfft(y).abs() ** self.power\n",
        "\n",
        "        return torch.nn.functional.l1_loss(x_mags, y_mags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTpOxfvY84hv"
      },
      "outputs": [],
      "source": [
        "def get_gain_animation(org, tgt, gains):\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    ax.plot(tgt[:200])\n",
        "    line,  = ax.plot([], [])\n",
        "    ax.set_xlabel(\"Time (samples)\")\n",
        "    ax.set_ylabel(\"Amplitude\")\n",
        "    ax.set_ylim(-1, 1)\n",
        "    ax.legend([\"Target\", \"Estimate\"], loc=1)\n",
        "\n",
        "    def init():\n",
        "        line.set_data([], [])\n",
        "        return line,\n",
        "\n",
        "    def animate(i):\n",
        "        line.set_data(np.arange(200), org[:200] * gains[i * 5])\n",
        "        ax.set_title(f\"Estimated signal after {i * 5} iterations\")\n",
        "        return line,\n",
        "\n",
        "    # Create the animation\n",
        "    anim = FuncAnimation(fig, animate, init_func=init, frames=len(gains) // 5, interval=50, blit=True)\n",
        "    plt.close(fig)\n",
        "    return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eraL99X684hv",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = LinearGain()\n",
        "\n",
        "spectral_loss = SpectralLoss()\n",
        "\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "n_iter = 300\n",
        "\n",
        "gains = []\n",
        "\n",
        "for i in range(n_iter):\n",
        "    gains.append(model.gain.item())\n",
        "\n",
        "    optim.zero_grad()\n",
        "    estim_audio = model(input_audio)\n",
        "    loss = spectral_loss(estim_audio, target_audio)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "display(HTML(get_gain_animation(input_audio, target_audio, gains).to_html5_video()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFTV4Zz584hv"
      },
      "source": [
        "More loss functions can be found in the PyTorch documentation https://pytorch.org/docs/stable/nn.html#loss-functions, while perceptual loss functions like spectral losses can be found in https://github.com/csteinmetz1/auraloss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyc2FpN384hv"
      },
      "source": [
        "### A simple <ins>*Differentiable*</ins> **IIR Filter** Effect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLFPiTeO84hv"
      },
      "source": [
        "A system with infinite impulse response (IIR) is called an IIR filter. Here, each processed output sample is dependent on both former samples of the input and former samples of the output, each scaled by a coefficient. It is also called a recursive system because the output samples are recursively computed from past output samples.\n",
        "\n",
        "The z-domain transfer function of a general second order IIR filter having 2 poles and 2 zeros (poles and zeros are the roots of the numerator and denominator of the transfer function, respectively), is given by:\n",
        "\n",
        "$\\begin{aligned}H(z) = \\frac{b_0 + b_1z^{-1} + b_2z^{-2}}{1 + a_1z^{-1} + a_2z^{-2}}\\end{aligned}$\n",
        "\n",
        "Since this transfer function is the ratio of two quadratic functions, it is commonly referred to as a biquad filter, which is used for many musical purposes.\n",
        "\n",
        "As before, we can train (automatically update and predict) the coefficients of this filter function to estimate a specific frequency response. We use the *Transposed Direct Form-II (TDF-II)* to retrieve the difference equation from above transfer function.\n",
        "\n",
        "- $y[n] = b_0x[n] + h_1[n-1]$\n",
        "- $h_1[n] = b_1x[n] - a_1y[n] + h_2[n-1]$\n",
        "- $h_2[n] = b_2x[n] - a_2y[n]$\n",
        "\n",
        "We again implement this difference equation into PyTorch by inhereting from the nn.Module.\n",
        "\n",
        "Notice how we store the vectors h1 and h2 in a matrix for simplification purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ljNWF0684hv"
      },
      "outputs": [],
      "source": [
        "class DTDFII(Module):\n",
        "    def __init__(self):\n",
        "        super(DTDFII, self).__init__()\n",
        "        self.b0 = Parameter(FloatTensor([uniform(-1, 1)]))\n",
        "        self.b1 = Parameter(FloatTensor([uniform(-1, 1)]))\n",
        "        self.b2 = Parameter(FloatTensor([uniform(-1, 1)]))\n",
        "        self.a1 = Parameter(FloatTensor([uniform(-0.5, 0.5)]))\n",
        "        self.a2 = Parameter(FloatTensor([uniform(-0.5, 0.5)]))\n",
        "\n",
        "    def _cat(self, vectors):\n",
        "        return torch.cat([v_.unsqueeze(-1) for v_ in vectors], dim=-1)\n",
        "\n",
        "    def forward(self, input, h):\n",
        "        output = input * self.b0 + h[:, 0]\n",
        "\n",
        "        h1 = input * self.b1 + h[:, 1] - output * self.a1\n",
        "        h2 = input * self.b2 - output * self.a2\n",
        "\n",
        "        h = self._cat([h1, h2])\n",
        "\n",
        "        return output, h\n",
        "\n",
        "    def init_states(self, size):\n",
        "        h = torch.zeros(size, 2).to(next(self.parameters()).device)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVCyLLM784hv"
      },
      "source": [
        "We define our input as a chirp (sine sweep) going from 0 to 20kHz in 10 seconds, with the target being the same sweep filtered by a DSP butterworth algorithm at 2kHz. By iteratively comparing the processed input to the actual filtered output, we try to adjust our differentiable IIR filter to match the frequency response of the original filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF94EX4e84hv"
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n",
        "import numpy as np\n",
        "\n",
        "fs = 48000\n",
        "sec = 10\n",
        "T = int(fs * sec)\n",
        "start_freq = 1\n",
        "end_freq = 20000\n",
        "t = np.linspace(0, sec, sec*fs)\n",
        "\n",
        "train_input = signal.chirp(t=t, f0=start_freq, t1=sec, f1=end_freq, method='logarithmic') + np.random.normal(scale=5e-2, size=len(t))\n",
        "\n",
        "fc = 18000 #Hz\n",
        "b, a = signal.butter(N=2, Wn=fc/fs, btype='high')\n",
        "print(\"The filter has the following coefficients:\")\n",
        "print(\"Coeffs b:\", b, \", coeffs a:\", a)\n",
        "sos = signal.tf2sos(b, a)\n",
        "\n",
        "train_target = signal.sosfilt(sos, train_input)\n",
        "impulse = np.zeros(1000)\n",
        "impulse[0] = 1.0\n",
        "imp_filter = signal.sosfilt(sos, impulse)\n",
        "\n",
        "plot_tf(\"Filtered chirp signal\", fs, np.arange(T) / fs, [train_target], [imp_filter])\n",
        "\n",
        "ipd.display(ipd.Audio(train_input, rate=fs, normalize=True))\n",
        "ipd.display(ipd.Audio(train_target, rate=fs, normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jccF3yLa84hv"
      },
      "source": [
        "Above training signal is 10 seconds long. At 48kHz, that is 480000 samples (that is a lot of data!). Performing our forward step on 480000 samples is computationally inefficient, meaning that we have to wait long for each gradient calculation and thus parameter update. In order to make the operations more efficient, we split our signals into batches of sequences, such that we now apply the filter operations on a matrix consisting of (batch_size, sequence_length, audio_channels).\n",
        "\n",
        "How this is done is not important, we use a utility function you can check out in the utils.py script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAvo6NfQ84hv"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "sequence_length = 512\n",
        "\n",
        "loader = DataLoader(dataset=DIIRDataSet(train_input, train_target, sequence_length), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "print(\"Batch dim:\", next(iter(loader))['input'].size())\n",
        "print(\"Sequences available in dataset:\", int(len(train_input)/sequence_length))\n",
        "print(\"Batches available in dataset:\", int(np.ceil(int(len(train_input)/sequence_length) / batch_size)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYyHT38s84hv"
      },
      "source": [
        "Above we see that from a training signal of 480000, we have 937 sequences of 512 samples. With a batch_size of 128, this means we have 8 batches: 7 consisting of 128 sequences, 1 consisting of 41 sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo3Xw-Jt84hv"
      },
      "source": [
        "We define our model, loss function and optimizer (this time we use Adam rather than SGD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O829pupj84hv"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "n_epochs = 5000\n",
        "\n",
        "filter_function = DTDFII()\n",
        "model = DIIR_WRAPPER(filter_function).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_FQjjN684hv"
      },
      "source": [
        "### Define training loop\n",
        "\n",
        "We define a training loop. Here we loop through each batch, calculate the loss and return it for visualisation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lZ_y0At84hw"
      },
      "outputs": [],
      "source": [
        "def train(criterion, model, loader, optimizer):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        input_seq_batch = batch['input'].to(device)\n",
        "        target_seq_batch = batch['target'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predicted_output = model(input_seq_batch)\n",
        "        loss = criterion(target_seq_batch, predicted_output)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    total_loss /= len(loader)\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5Bo_Jv84hw"
      },
      "source": [
        "### Train!\n",
        "#### BE AWARE - ON A CPU, TRAINING MAY TAKE SEVERAL HOURS (TOOK 45 min ON A GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82YMqfa184hw"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "b0, b1, b2 = [], [], []\n",
        "a1, a2 = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    loss = train(criterion, model, loader, optimizer)\n",
        "    losses.append(loss)\n",
        "\n",
        "    b0.append(model.cell.b0.item())\n",
        "    b1.append(model.cell.b1.item())\n",
        "    b2.append(model.cell.b2.item())\n",
        "    a1.append(model.cell.a1.item())\n",
        "    a2.append(model.cell.a2.item())\n",
        "\n",
        "    if epoch %200 == 0:\n",
        "        print(\"Epoch {} -- Loss {:3E}\".format(epoch, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuA71XRz84hw"
      },
      "outputs": [],
      "source": [
        "coeffs = [b0, b1, b2, a1, a2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iCfdj8L84hw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEsaZhck84hw"
      },
      "source": [
        "Let's look at how each coefficient adjusts across each iteration.\n",
        "\n",
        "We clearly see that they are finding their way towards a configuration that gives the minimum of our objective function - this is the magic of DDSP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVXXEFb584hw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Animate the fitting process\n",
        "def get_loss_animation(losses_list):\n",
        "    num_iterations = len(losses_list[0])\n",
        "    num_plots = len(losses_list)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "    lines_loss = []\n",
        "    for i in range(num_plots):\n",
        "        if i < 3:\n",
        "            label=f\"b{i+1}\"\n",
        "        else:\n",
        "            label=f\"a{i-3}\"\n",
        "        line_loss, = ax.plot([], [], lw=2, label=label)\n",
        "        lines_loss.append(line_loss)\n",
        "\n",
        "    ax.set_xlim(0, num_iterations)\n",
        "    ax.set_ylim(-1, max([max(losses) for losses in losses_list]))\n",
        "    ax.set_xlabel(\"Iteration\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.legend()\n",
        "\n",
        "    def init():\n",
        "        for line in lines_loss:\n",
        "            line.set_data([], [])\n",
        "        return lines_loss\n",
        "\n",
        "    def animate(iter_idx):\n",
        "        for i, line_loss in enumerate(lines_loss):\n",
        "            line_loss.set_data(np.arange((iter_idx*30)+1), losses_list[i][:(iter_idx*30)+1])\n",
        "            ax.set_title(f\"Params after {iter_idx*30} iterations\")\n",
        "        return lines_loss\n",
        "\n",
        "    # Create the animation\n",
        "    anim = FuncAnimation(fig, animate, init_func=init, frames=num_iterations // 30, interval=50, blit=True)\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "# Example usage:\n",
        "display(HTML(get_loss_animation(coeffs).to_html5_video()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DvL8LF184hw"
      },
      "source": [
        "Let's also see if we have approached the target frequency response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RiCe3LB84hw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "impulse = np.zeros(sequence_length)\n",
        "impulse[0] = 1.0\n",
        "impulse = torch.tensor(impulse, dtype=torch.float32).to(device)\n",
        "\n",
        "input_to_process = train_input\n",
        "padding = int(np.ceil((len(input_to_process) / sequence_length)) * sequence_length) - len(input_to_process)\n",
        "batched_input = torch.nn.functional.pad(torch.tensor(input_to_process, dtype=torch.float32), (0, padding)).reshape(-1, sequence_length, 1)\n",
        "processed = torch.zeros(batched_input.shape)\n",
        "\n",
        "with torch.no_grad():\n",
        "    processed = model(batched_input.to(device))\n",
        "    imp_model = model(impulse.unsqueeze(0).unsqueeze(-1))\n",
        "\n",
        "processed = processed.reshape(-1).cpu().numpy()\n",
        "imp_model = imp_model.reshape(-1).cpu().numpy()\n",
        "\n",
        "plot_tf(\n",
        "    \"Filtered Chirp signal\",\n",
        "    fs,\n",
        "    np.arange(len(train_target)) / fs,\n",
        "    [train_target, processed[:len(train_target)]],\n",
        "    [imp_filter, imp_model],\n",
        "    [\"scipy.signal.butter\", \"diff_iir\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muvepy5M84hw"
      },
      "source": [
        "We see that we are approaching the target frequency reponse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MmGPxQ684hw"
      },
      "source": [
        "## TASKS for Further Experimentation\n",
        "Below are two tasks.\n",
        "\n",
        "a) First, we create another differentiable filter that more freely can predict frequency responses. You are tasked to experiment with the filter, trying to create a target signal and add the filter model to the wrapper. Lastly, you can try to train the filter (I recommend doing this in a notebook).\n",
        "\n",
        "b) Secondly we provide an implementation and training scheme for the wave equation in a differentiable manner. Many of you know the wave equation from other courses (i.e. the karplus-strong algorithm). You will see how we can use differentiable signal processing to approximate damping coefficients and other physical parameters to obtain a target sound.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4naqE3L84hw"
      },
      "source": [
        "### a) State Variable Filter (SVF)\n",
        "\n",
        "The above filter problem was tailored to work. As seen in the implementation of the DTDFII module, we clamp the coefficients for stability reasons. This means that not all coefficient configurations are possible and thus not all 2nd order frequency responses can be obtained. A high-pass filter with a cutoff at 18kHz was thus deliberately chosen as I knew the DTDFII would be able to find the respective coefficients.\n",
        "\n",
        "To take account of this, and to be able to predict 2nd order filter frequency responses more freely, we can use the State-Variable Filter (SVF). The SVF can produce any second-order transfer function, whilst still having easily interpretable parameters. Its difference equation is given by:\n",
        "\n",
        "$\\begin{aligned} y_{\\mathrm{BP}}[n] & =\\frac{g\\left(x[n]-h_2[n-1]\\right)+h_1[n-1]}{1+g(g+2 R)} \\\\ y_{\\mathrm{LP}}[n] & =g y_{\\mathrm{BP}}[n]+h_2[n-1] \\\\ y_{\\mathrm{HP}}[n] & =x[n]-y_{\\mathrm{LP}}[n]-2 R y_{\\mathrm{BP}}[n] \\\\ h_1[n] & =2 y_{\\mathrm{BP}}-h_1[n-1] \\\\ h_2[n] & =2 y_{\\mathrm{LP}}-h_2[n-1] \\\\ y[n] & =c_{\\mathrm{HP}} y_{\\mathrm{HP}}+c_{\\mathrm{BP}} y_{\\mathrm{BP}}+c_{\\mathrm{LP}} y_{\\mathrm{LP}},\\end{aligned}$\n",
        "\n",
        "With the parameters being:\n",
        "\n",
        "- cHP = high-pass mixing coefficient\n",
        "- cBP = band-pass mixing coefficient\n",
        "- cLP = low-pass mixing coefficient\n",
        "- R = damping/resonance\n",
        "- g = frequency cutoff\n",
        "\n",
        "We will not go into technical details with the SVF and you do not need to understand the math behind it. However, it is good to be aware of each parameters functionality. Anyone interested in further details can read more about the SVF here: https://www.dafx14.fau.de/papers/dafx14_aaron_wishnick_time_varying_filters_for_.pdf\n",
        "\n",
        "In the following section, you will be tasked to implement the SVF into the differentiable framework and train it to match a specific frequency response. The SVF implementation and most of the needed code will be provided, you are asked to fill in the empty spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNYPnhrr84hw"
      },
      "outputs": [],
      "source": [
        "class DSVF(Module):\n",
        "    def __init__(self, G=0.5, twoR=1, hp_gain=0.0, bp_gain=0.0, lp_gain=1.0):\n",
        "        args = locals()\n",
        "        del args['self']\n",
        "        del args['__class__']\n",
        "        super(DSVF, self).__init__()\n",
        "        for key in args:\n",
        "            setattr(self, key, Parameter(FloatTensor([args[key]])))\n",
        "        self.master_gain = Parameter(FloatTensor([1.0]))\n",
        "\n",
        "    def forward(self, x, v):\n",
        "        coeff0, coeff1 = self.calc_coeffs()\n",
        "        input_minus_v1 = x - v[:, 1]\n",
        "        bp_out = coeff1 * input_minus_v1 + coeff0 * v[:, 0]\n",
        "        lp_out = self.G * bp_out + v[:, 1]\n",
        "        hp_out = x - lp_out - self.twoR * bp_out\n",
        "        v = torch.cat([(2 * bp_out).unsqueeze(-1), (2 * lp_out).unsqueeze(-1)], dim=-1) - v\n",
        "        y = self.master_gain * (self.hp_gain * hp_out + self.bp_gain * self.twoR * bp_out + self.lp_gain * lp_out)\n",
        "        return y, v\n",
        "\n",
        "    def init_states(self, size):\n",
        "        v = torch.zeros(size, 2).to(next(self.parameters()).device)\n",
        "        return v\n",
        "\n",
        "    def calc_coeffs(self):\n",
        "        self.G.data = torch.clamp(self.G, min=1e-8)\n",
        "        self.twoR.data = torch.clamp(self.twoR, min=0)\n",
        "        self.bp_gain.data = torch.clamp(self.bp_gain, min=-1)\n",
        "        self.hp_gain.data = torch.clamp(self.hp_gain, min=-1, max=1)\n",
        "        self.lp_gain.data = torch.clamp(self.lp_gain, min=-1, max=1)\n",
        "\n",
        "        coeff0 = 1.0 / (1.0 + self.G * (self.G + self.twoR))\n",
        "        coeff1 = self.G * coeff0\n",
        "\n",
        "        return coeff0, coeff1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkcyEbJV84hw"
      },
      "source": [
        "### Create input and target training signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASnI0cJY84hw"
      },
      "outputs": [],
      "source": [
        "fs = 48000\n",
        "sec = 2\n",
        "T = int(fs * sec)\n",
        "start_freq = 1\n",
        "end_freq = 20000\n",
        "t = np.linspace(0, sec, sec*fs)\n",
        "\n",
        "train_input = signal.chirp(t=t, f0=start_freq, t1=sec, f1=end_freq, method='logarithmic') + np.random.normal(scale=5e-2, size=len(t))\n",
        "\n",
        "fc = #choose filter cutoff\n",
        "filter_type = #choose filter type\n",
        "b, a = signal.butter(N=2, Wn=fc/fs, btype=filter_type)\n",
        "\n",
        "print(\"The filter has the following coefficients:\")\n",
        "print(\"Coeffs b:\", b, \", coeffs a:\", a)\n",
        "sos = signal.tf2sos(b, a)\n",
        "\n",
        "train_target = signal.sosfilt(sos, train_input)\n",
        "impulse = np.zeros(1000)\n",
        "impulse[0] = 1.0\n",
        "imp_filter = signal.sosfilt(sos, impulse)\n",
        "\n",
        "plot_tf(\"Filtered chirp signal\", fs, np.arange(T) / fs, [train_target], [imp_filter])\n",
        "\n",
        "ipd.display(ipd.Audio(train_input, rate=fs, normalize=True))\n",
        "ipd.display(ipd.Audio(train_target, rate=fs, normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRN3czwa84hx"
      },
      "outputs": [],
      "source": [
        "batch_size = #choose batch size\n",
        "sequence_length = #choose sequence length\n",
        "\n",
        "loader = DataLoader(dataset=DIIRDataSet(train_input, train_target, sequence_length), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "print(\"Batch dim:\", next(iter(loader))['input'].size())\n",
        "print(\"Sequences available in dataset:\", int(len(train_input)/sequence_length))\n",
        "print(\"Batches available in dataset:\", int(np.ceil(int(len(train_input)/sequence_length) / batch_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1jTgU1Z84hx"
      },
      "outputs": [],
      "source": [
        "n_epochs = 1500\n",
        "\n",
        "filter_function = #initialise differentiable filter\n",
        "model = #add to wrapper\n",
        "optimizer = #initialise optimizer\n",
        "criterion = #initialise loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2mUPXwK84hx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#train the model\n",
        "for epoch in range(n_epochs):\n",
        "    loss = #train\n",
        "    losses.append(loss)\n",
        "\n",
        "    if epoch %200 == 0:\n",
        "        print(\"Epoch {} -- Loss {:3E}\".format(epoch, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4JgiQ0B84hx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "impulse = np.zeros(sequence_length)\n",
        "impulse[0] = 1.0\n",
        "impulse = torch.tensor(impulse, dtype=torch.float32).to(device)\n",
        "\n",
        "input_to_process = train_input\n",
        "padding = int(np.ceil((len(input_to_process) / sequence_length)) * sequence_length) - len(input_to_process)\n",
        "batched_input = torch.nn.functional.pad(torch.tensor(input_to_process, dtype=torch.float32), (0, padding)).reshape(-1, sequence_length, 1)\n",
        "processed = torch.zeros(batched_input.shape)\n",
        "\n",
        "with torch.no_grad():\n",
        "    processed = model(batched_input.to(device))\n",
        "    imp_model = model(impulse.unsqueeze(0).unsqueeze(-1))\n",
        "\n",
        "processed = processed.reshape(-1).cpu().numpy()\n",
        "imp_model = imp_model.reshape(-1).cpu().numpy()\n",
        "\n",
        "\n",
        "plot_tf(\n",
        "    \"Filtered Chirp signal\",\n",
        "    fs,\n",
        "    np.arange(len(train_target)) / fs,\n",
        "    [train_target, processed[:len(train_target)]],\n",
        "    [imp_filter, imp_model],\n",
        "    [\"scipy.signal.butter\", \"diff_iir\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c35jdip584hx"
      },
      "source": [
        "### b) The Wave Equation\n",
        "\n",
        "In this section, we'll look at physical sound synthesis and model a string sound from the wave equation. Thereafter we will use gradient descent to find the parameters of the model that best fit a given sound.\n",
        "\n",
        "In particular we will focus on digital waveguide synthesis (DWG). DWGs are based on D'Alembert's [travelling wave solution](https://en.wikipedia.org/wiki/D%27Alembert%27s_formula) to the wave equation, where the solution is given by waves travelling on opposite directions:\n",
        "\n",
        "$$\n",
        "u(x, t) = F(x + ct) + G(x - ct)\n",
        "$$\n",
        "\n",
        "here $F(x + ct)$ represents a wave traveling to the left and $G(x - ct)$ represents a wave traveling to the right.\n",
        "\n",
        "In DWGs, the propagation of the traveling waves is simulated using delay lines. At each sample step, losses occur, but if the loss is a linear operation, it can be commuted out of the individual samples and be applied cumulatively to the output of the delay line.\n",
        "\n",
        "The model of the loss should be frequency-dependent. With the simplest possible loss filter, we obtain a simulation diagram that looks like this:\n",
        "\n",
        "<div>\n",
        "<img src=\"./img/kp-strong.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "This might look familiar as the basic structure of the Karplus-Strong algorithm for plucked string synthesis. In fact, the Karplus-Strong algorithm can be seen as a simple DWG. We'll look at applying the same methods as before to find the parameters of this model that best fit a given sound using gradient descent.\n",
        "\n",
        "\n",
        "The transfer function of the basic Karplus-Strong algorithm as shown before is\n",
        "\n",
        "$$H(z) = \\frac{1}{1 - g\\cdot(z^{-N} + z^{-N-1})},$$\n",
        "\n",
        "where $N$ is the length of the delay line corresponding to the modeled string and controls pitch, and $g$ is the feedback gain, which controls the decay time of the sound.\n",
        "\n",
        "We'll implement this transfer function in the frequency domain for more efficient estimation, and in the time domain for the final result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuzd2OFm84hx"
      },
      "outputs": [],
      "source": [
        "class KarplusStrong(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, delay_len, n_fft=2048):\n",
        "        super().__init__()\n",
        "        self.delay_gain = torch.nn.Parameter(torch.tensor(0.0))\n",
        "        self.delay_len = delay_len\n",
        "\n",
        "        # for frequency sampling\n",
        "        self.z = torch.exp(1j * torch.linspace(0, torch.pi, n_fft // 2 + 1))\n",
        "\n",
        "        # random excitation\n",
        "        self.exc = torch.zeros(n_fft)\n",
        "        self.exc[:delay_len] = torch.rand(delay_len) - 0.5\n",
        "        self.exc_fft = torch.fft.rfft(self.exc)\n",
        "\n",
        "    # scale delay gain to [0.9, 1.0]\n",
        "    def scaled_gain(self):\n",
        "        return torch.sigmoid(self.delay_gain) * 0.1 + 0.9\n",
        "\n",
        "    # forward pass: synthesis in the frequency domain\n",
        "    def forward(self):\n",
        "        z = self.z\n",
        "\n",
        "        delay_gain = self.scaled_gain()\n",
        "\n",
        "        # sample transfer function\n",
        "        numer = 1.\n",
        "        denom = (1 - delay_gain * (0.5 * z ** (-self.delay_len) + 0.5 * z ** (-self.delay_len - 1)))\n",
        "\n",
        "        # filter excitation in frequency domain\n",
        "        return self.exc_fft * numer / denom\n",
        "\n",
        "    # also provide method for time domain synthesis\n",
        "    def time_domain_synth(self, n_samples):\n",
        "\n",
        "        delay_gain = self.scaled_gain()\n",
        "\n",
        "        # populate filter coefficients for IIR filter\n",
        "        a_coeffs = torch.zeros(self.delay_len + 2)\n",
        "        a_coeffs[0] = 1\n",
        "        a_coeffs[self.delay_len] = -delay_gain * 0.5\n",
        "        a_coeffs[self.delay_len + 1] = -delay_gain * 0.5\n",
        "\n",
        "        b_coeffs = torch.zeros(self.delay_len + 2)\n",
        "        b_coeffs[0] = 1\n",
        "\n",
        "        # pad or truncate self.exc to n_samples\n",
        "        if self.exc.shape[0] < n_samples:\n",
        "            audio = torch.cat([self.exc, torch.zeros(n_samples - self.exc.shape[0])])\n",
        "        else:\n",
        "            audio = self.exc[:n_samples]\n",
        "\n",
        "        audio = torchaudio.functional.lfilter(audio, a_coeffs, b_coeffs, clamp=False)\n",
        "        return audio\n",
        "\n",
        "# let's have a listen\n",
        "synth = KarplusStrong(80)\n",
        "audio = synth.time_domain_synth(32000)\n",
        "ipd.Audio(audio.detach(), rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6GHHVOV84hx"
      },
      "source": [
        "Let's now load an acoustic guitar sound file from the NSynth dataset. We'll try to have our Karplus-Strong model mimic this sound. Since it is a very simple model, we won't get too close of a match, but we should be able to tune the decay time.\n",
        "\n",
        "As mentioned before, pitch estimation with gradient descent can be tricky, so we'll infer the length of the delay line from the pitch of the recording: At MIDI note 51, it's about 155.56 Hz. With a sample rate of 16000 Hz, this corresponds to a delay of 102.8 samples. We'll round this to 103 samples. More accuracy could be achieved by using fractional delays, but we'll keep it simple here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiF0UIQ284hx"
      },
      "outputs": [],
      "source": [
        "sr = 16000\n",
        "\n",
        "audio, sr = librosa.load(\"sound-files/guitar-nsynth.wav\", sr=sr, mono=True)\n",
        "\n",
        "# how many points used in sampling the transfer function\n",
        "nfft = 4096\n",
        "\n",
        "# fix random excitation\n",
        "torch.manual_seed(0)\n",
        "\n",
        "karplus_model = KarplusStrong(delay_len=103, n_fft=nfft)\n",
        "\n",
        "print(\"Original:\")\n",
        "ipd.display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "print(\"Synthesized:\")\n",
        "ipd.display(ipd.Audio(karplus_model.time_domain_synth(sr * 4).detach(), rate=sr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjz2PIcm84hx"
      },
      "source": [
        "This doesn't sound close at all. Let's see if we can once again use gradient descent to find a better value for $g$ and match the decay time. We'll define our own loss function using L1 loss on the normalized log magnitudes of the spectrum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZkkt9DA84hx"
      },
      "outputs": [],
      "source": [
        "def to_log_mag(freq_response, rel_to_max=True, eps=1e-7):\n",
        "    mag = torch.abs(freq_response)\n",
        "    if rel_to_max:\n",
        "        div = torch.max(mag)\n",
        "    else:\n",
        "        div = 1.0\n",
        "    return 10 * torch.log10(mag / div + eps)\n",
        "\n",
        "\n",
        "def loss_fn(y, y_hat):\n",
        "    y_mags = to_log_mag(y)\n",
        "    y_hat_mags = to_log_mag(y_hat)\n",
        "\n",
        "    return torch.mean((y_mags - y_hat_mags).abs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSr5Y5Ag84hx"
      },
      "source": [
        "We're all set for optimization!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tShBo6DD84hx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# calculate truncated fft\n",
        "target = torch.fft.rfft(torch.tensor(audio), n=nfft)\n",
        "\n",
        "fftfreqs = torch.fft.rfftfreq(nfft, 1 / sr)\n",
        "\n",
        "plt.plot(fftfreqs, to_log_mag(target.detach()), label=\"target\")\n",
        "plt.plot(fftfreqs, to_log_mag(karplus_model().detach()), label=\"initial synthesis\")\n",
        "\n",
        "optim = torch.optim.Adam(karplus_model.parameters(), lr=1e-2)\n",
        "for i in range(1000):\n",
        "    optim.zero_grad()\n",
        "    loss = loss_fn(target, karplus_model())\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "plt.plot(fftfreqs, to_log_mag(karplus_model().detach()), label=\"optimized synthesis\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Magnitude (dB)\")\n",
        "plt.xlabel(\"Frequency (Hz)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Audio after optimization:\")\n",
        "td_out = karplus_model.time_domain_synth(audio.shape[0]).detach()\n",
        "ipd.display(ipd.Audio(td_out, rate=sr))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}