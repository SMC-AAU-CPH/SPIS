{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Audio playback\n\nThis notebook demonstrates how to use IPython's audio playback\nto play audio signals through your web browser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Code source: Brian McFee\n# License: ISC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll need numpy and matplotlib for this example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n# sphinx_gallery_thumbnail_path = '_static/playback-thumbnail.png'\n\nimport librosa\n\n# We'll need IPython.display's Audio widget\nfrom IPython.display import Audio\n\n# We'll also use `mir_eval` to synthesize a signal for us\nimport mir_eval.sonify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playing a synthetic sound\nThe IPython Audio widget accepts raw numpy data as\naudio signals.  This means we can synthesize signals\ndirectly and play them back in the browser.\n\nFor example, we can make a sine sweep from C3 to C5:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sr = 22050\n\ny_sweep = librosa.chirp(fmin=librosa.note_to_hz('C3'),\n                        fmax=librosa.note_to_hz('C5'),\n                        sr=sr,\n                        duration=1)\n\nAudio(data=y_sweep, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playing a real sound\nOf course, we can also play back real recorded sounds\nin the same way.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y, sr = librosa.load(librosa.ex('trumpet'))\n\nAudio(data=y, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sonifying pitch estimates\nAs a slightly more advanced example, we can\nuse sonification to directly observe the output of a\nfundamental frequency estimator.\n\nWe'll do this using `librosa.pyin` for analysis,\nand `mir_eval.sonify.pitch_contour` for synthesis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Using fill_na=None retains the best-guess f0 at unvoiced frames\nf0, voiced_flag, voiced_probs = librosa.pyin(y,\n                                             sr=sr,\n                                             fmin=librosa.note_to_hz('C2'),\n                                             fmax=librosa.note_to_hz('C7'),\n                                             fill_na=None)\n\n# To synthesize the f0, we'll need sample times\ntimes = librosa.times_like(f0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "mir_eval's synthesizer uses negative f0 values to indicate\nunvoiced regions.\n\nWe'll make an array vneg which is 1 for voiced frames, and\n-1 for unvoiced frames.\nThis way, `f0 * vneg` will leave voiced estimates unchanged,\nand negate the frequency for unvoiced frames.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vneg = (-1)**(~voiced_flag)\n\n# And sonify the f0 using mir_eval\ny_f0 = mir_eval.sonify.pitch_contour(times, f0 * vneg, sr)\n\nAudio(data=y_f0, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sonifying mixtures\nFinally, we can also use the Audio widget to listen\nto combinations of signals.\n\nThis example runs the onset detector over the original\ntest clip, and then synthesizes a click at each detection.\n\nWe can then overlay the click track on the original signal\nand hear them both.\n\nFor this to work, we need to ensure that both the synthesized\nclick track and the original signal are of the same length.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute the onset strength envelope, using a max filter of 5 frequency bins\n# to cut down on false positives\nonset_env = librosa.onset.onset_strength(y=y, sr=sr, max_size=5)\n\n# Detect onset times from the strength envelope\nonset_times = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, units='time')\n\n# Sonify onset times as clicks\ny_clicks = librosa.clicks(times=onset_times, length=len(y), sr=sr)\n\nAudio(data=y+y_clicks, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caveats\nFinally, some important things to note when using interactive playback:\n\n  - `IPython.display.Audio` works by serializing the entire audio signal and\n    sending it to the browser in a UUEncoded stream.  This may be inefficient for\n    long signals.\n  - `IPython.display.Audio` can also work directly with filenames and URLs.  If\n    you're working with long signals, or do not want to load the signal into python\n    directly, it may be better to use one of these modes.\n  - Audio playback, by default, will normalize the amplitude of the signal being\n    played.  Most of the time this is what you will want, but sometimes it may not\n    be, so be aware that normalization can be disabled.\n  - If you're working in a Jupyter notebook and want to show multiple Audio widgets\n    in the same cell, you can use\n    ``IPython.display.display(IPython.display.Audio(...))`` to explicitly render\n    each widget.  This is helpful when playing back multiple related signals.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}